{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MorseGen\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "morse_gen = MorseGen.Morse()\n",
    "alphabet = morse_gen.alphabet36\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create string for dataset\n",
    "def get_training_samples():\n",
    "    return MorseGen.get_morse_str(nchars=132*6, nwords=27*6, chars=alphabet)\n",
    "\n",
    "# Get database with morse code parameter\n",
    "def get_new_data(morse_gen, SNR_dB=-23, nchars=132*6, nwords=27*6, phrase=None, alphabet=\"ABC\"):\n",
    "    if not phrase:\n",
    "        phrase = MorseGen.get_morse_str(nchars=nchars, nwords=nwords, chars=alphabet)\n",
    "    Fs = 6000\n",
    "    samples_per_dit = morse_gen.nb_samples_per_dit(Fs, 20)\n",
    "    n_prev = int((samples_per_dit/128)*55) + 1 \n",
    "    label_df = morse_gen.encode_df_decim_tree(phrase, samples_per_dit, 128, alphabet)\n",
    "    envelope = label_df['env'].to_numpy()\n",
    "    label_df.drop(columns=['env'], inplace=True)\n",
    "    SNR_linear = 10.0**(SNR_dB/10.0)  \n",
    "    SNR_linear *= 256 # Apply original FFT\n",
    "    t = np.linspace(0, len(envelope)-1, len(envelope))\n",
    "    power = np.sum(envelope**2)/len(envelope)\n",
    "    print(power)\n",
    "    noise_power = power/SNR_linear\n",
    "    noise = np.sqrt(noise_power)*np.random.normal(0, 1, len(envelope))\n",
    "    # noise = butter_lowpass_filter(raw_noise, 0.9, 3) # Noise is also filtered in the original setup from audio. This empirically simulates it\n",
    "    signal = (envelope + noise)**2\n",
    "    #signal[signal > 1.0] = 1.0 # a bit crap ...\n",
    "    return envelope, signal, label_df, n_prev\n",
    "\n",
    "# Define class for morse dataset\n",
    "class MorsekeyingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, morse_gen, device, SNR_dB=-23, nchars=132, nwords=27, phrase=None, alphabet=\"ABC\"):\n",
    "        self.envelope, self.signal, self.label_df0, self.seq_len = get_new_data(morse_gen, SNR_dB=SNR_dB, phrase=phrase, alphabet=alphabet)\n",
    "        self.label_df = self.label_df0.drop(columns=['dit','dah'])\n",
    "        self.X = torch.FloatTensor(self.signal).to(device)\n",
    "        self.y = torch.FloatTensor(self.label_df.values).to(device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.__len__() - self.seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.X[index:index+self.seq_len], self.y[index+self.seq_len])\n",
    "    \n",
    "    def get_envelope(self):\n",
    "        return self.envelope\n",
    "    \n",
    "    def get_signal(self):\n",
    "        return self.signal\n",
    "    \n",
    "    def get_X(self):\n",
    "        return self.X\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return self.label_df\n",
    "    \n",
    "    def get_labels0(self):\n",
    "        return self.label_df0\n",
    "    \n",
    "    def get_seq_len(self):\n",
    "        return self.seq_len()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Concat_training_dataset():\n",
    "    SNR = [-21, -20, -19, -18, -17, -16, -15, -10]\n",
    "    train_chr_dataset = []\n",
    "    for value in SNR:\n",
    "        morse_str = get_training_samples()\n",
    "        train_chr_dataset0 = MorsekeyingDataset(morse_gen, device, value, 132*5, 27*5, morse_str, alphabet)\n",
    "        train_chr_dataset = torch.utils.data.ConcatDataset([train_chr_dataset, train_chr_dataset0])\n",
    "    return train_chr_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Concat_training_loader(train_chr_dataset):\n",
    "    return torch.utils.data.DataLoader(train_chr_dataset, batch_size=1, shuffle=True) # Batch size must be 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorseBatchedMultiLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Initial implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, device, input_size=1, hidden_layer1_size=6, output1_size=6, hidden_layer2_size=12, output_size=14):\n",
    "        super().__init__()\n",
    "        self.device = device # This is the only way to get things work properly with device\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layer1_size = hidden_layer1_size\n",
    "        self.output1_size = output1_size\n",
    "        self.hidden_layer2_size = hidden_layer2_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_layer1_size)\n",
    "        self.linear1 = nn.Linear(hidden_layer1_size, output1_size)\n",
    "        self.hidden1_cell = (torch.zeros(1, 1, self.hidden_layer1_size).to(self.device),\n",
    "                             torch.zeros(1, 1, self.hidden_layer1_size).to(self.device))\n",
    "        self.lstm2 = nn.LSTM(input_size=output1_size, hidden_size=hidden_layer2_size)\n",
    "        self.linear2 = nn.Linear(hidden_layer2_size, output_size)\n",
    "        self.hidden2_cell = (torch.zeros(1, 1, self.hidden_layer2_size).to(self.device),\n",
    "                             torch.zeros(1, 1, self.hidden_layer2_size).to(self.device))\n",
    "    \n",
    "    def _minmax(self, x):\n",
    "        x -= x.min(0)[0]\n",
    "        x /= x.max(0)[0]\n",
    "        \n",
    "    def _hardmax(self, x):\n",
    "        x /= x.sum()\n",
    "        \n",
    "    def _sqmax(self, x):\n",
    "        x = x**2\n",
    "        x /= x.sum()\n",
    "        \n",
    "    def forward(self, input_seq):\n",
    "        #print(len(input_seq), input_seq.shape, input_seq.view(-1, 1, 1).shape)\n",
    "        lstm1_out, self.hidden1_cell = self.lstm1(input_seq.view(-1, 1, self.input_size), self.hidden1_cell)\n",
    "        pred1 = self.linear1(lstm1_out.view(len(input_seq), -1))\n",
    "        lstm2_out, self.hidden2_cell = self.lstm2(pred1.view(-1, 1, self.output1_size), self.hidden2_cell)\n",
    "        predictions = self.linear2(lstm2_out.view(len(pred1), -1))\n",
    "        self._sqmax(predictions[-1])\n",
    "        return predictions[-1]\n",
    "    \n",
    "    def zero_hidden_cell(self):\n",
    "        self.hidden1_cell = (\n",
    "            torch.zeros(1, 1, self.hidden_layer1_size).to(device),\n",
    "            torch.zeros(1, 1, self.hidden_layer1_size).to(device)\n",
    "        )     \n",
    "        self.hidden2_cell = (\n",
    "            torch.zeros(1, 1, self.hidden_layer2_size).to(device),\n",
    "            torch.zeros(1, 1, self.hidden_layer2_size).to(device)\n",
    "        )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5292790583619421\n",
      "0.5280647496216359\n",
      "0.52938825801725\n",
      "0.5319580877537655\n",
      "0.5376608262752958\n",
      "0.5330677030601914\n",
      "0.5262804366078925\n",
      "0.5273577032214989\n"
     ]
    }
   ],
   "source": [
    "train_chr_dataset = get_Concat_training_dataset()\n",
    "train_chr_loader = get_Concat_training_loader(train_chr_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "morse_chr_model = MorseBatchedMultiLSTM(device, hidden_layer1_size=12, output1_size=4, hidden_layer2_size=len(alphabet)*2, output_size=len(alphabet)+4).to(device) # This is the only way to get things work properly with device\n",
    "morse_chr_loss_function = nn.MSELoss()\n",
    "morse_chr_optimizer = torch.optim.Adam(morse_chr_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_chr_dataset = get_Concat_training_dataset()\n",
    "    train_chr_loader = get_Concat_training_loader(train_chr_dataset)\n",
    "\n",
    "    epochs = 4\n",
    "\n",
    "    morse_chr_model = MorseBatchedMultiLSTM(device, hidden_layer1_size=12, output1_size=4, hidden_layer2_size=len(alphabet)*2, output_size=len(alphabet)+4).to(device) # This is the only way to get things work properly with device\n",
    "    morse_chr_loss_function = nn.MSELoss()\n",
    "    morse_chr_optimizer = torch.optim.Adam(morse_chr_model.parameters(), lr=0.001)\n",
    "\n",
    "    # morse_chr_model.train()\n",
    "\n",
    "    # for i in range(epochs):\n",
    "    #     train_losses = []\n",
    "    #     loop = tqdm(enumerate(train_chr_loader), total=len(train_chr_loader), leave=True)\n",
    "    #     for j, train in loop:\n",
    "    #         X_train = train[0][0]\n",
    "    #         y_train = train[1][0]\n",
    "    #         # Xóa Gradient\n",
    "    #         morse_chr_optimizer.zero_grad()\n",
    "    #         if morse_chr_model.__class__.__name__ in [\"MorseLSTM\", \"MorseLSTM2\", \"MorseBatchedLSTM\", \"MorseBatchedLSTM2\", \"MorseBatchedMultiLSTM\", \"MorseBatchedLSTMLin2\"]:\n",
    "    #             morse_chr_model.zero_hidden_cell() # this model needs to reset the hidden cell\n",
    "    #         y_pred = morse_chr_model(X_train)\n",
    "    #         # Tính toán lỗi và thực hiện backpropagation\n",
    "    #         single_loss = morse_chr_loss_function(y_pred, y_train)\n",
    "    #         single_loss.backward()\n",
    "    #         # Cập nhật trọng số\n",
    "    #         morse_chr_optimizer.step()\n",
    "    #         train_losses.append(single_loss.item())\n",
    "    #         # update progress bar\n",
    "    #         if j % 1000 == 0:\n",
    "    #             loop.set_description(f\"Epoch [{i+1}/{epochs}]\")\n",
    "    #             loop.set_postfix(loss=np.mean(train_losses))\n",
    "\n",
    "    # print(f'final: {i+1:3} epochs loss: {np.mean(train_losses):6.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(morse_chr_model.state_dict(), '/models/morse_a26_model.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morse_chr_model.load_state_dict(torch.load('morse_a26_3_model', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io.wavfile\n",
    "import torch\n",
    "from scipy import signal\n",
    "from scipy.io.wavfile import write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate, data = scipy.io.wavfile.read(\"morse.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.FloatTensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chr_dataset = MorsekeyingDataset(morse_gen, device, -18, 132*5, 27*5, teststr, alphabet)\n",
    "test_chr_loader = torch.utils.data.DataLoader(data, batch_size=1, shuffle=False) # Batch size must be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0f8be83a9d4cea938525356e6e19bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27672 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Khoi Nghiem Folder\\Final project\\test.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Khoi%20Nghiem%20Folder/Final%20project/test.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Khoi%20Nghiem%20Folder/Final%20project/test.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     X_test \u001b[39m=\u001b[39m test[\u001b[39m0\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Khoi%20Nghiem%20Folder/Final%20project/test.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     pred_val \u001b[39m=\u001b[39m morse_chr_model(X_test[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Khoi%20Nghiem%20Folder/Final%20project/test.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     p_chr_test \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([p_chr_test, pred_val\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m,\u001b[39mlen\u001b[39m(alphabet)\u001b[39m+\u001b[39m\u001b[39m4\u001b[39m)])\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
     ]
    }
   ],
   "source": [
    "p_chr_test = torch.empty(1,len(alphabet)+4).to(device)\n",
    "morse_chr_model.eval()\n",
    "\n",
    "loop = tqdm(enumerate(test_chr_loader), total=len(test_chr_loader))\n",
    "for j, test in loop:\n",
    "    with torch.no_grad():\n",
    "        X_test = test[0]\n",
    "        pred_val = morse_chr_model(X_test[0])\n",
    "        p_chr_test = torch.cat([p_chr_test, pred_val.reshape(1,len(alphabet)+4)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b6cac89ce16068980d92779c002f85bcfea9934a17a862361f48e3b487b7df04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
